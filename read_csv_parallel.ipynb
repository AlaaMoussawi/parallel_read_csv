{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to parallelize read_csv\n",
    "\n",
    "# To be used with split files generated by script_to_split.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import norm\n",
    "\n",
    "import calendar\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.43 ms\n"
     ]
    }
   ],
   "source": [
    "def read_csv(filename):\n",
    "    'converts a filename to a pandas dataframe'\n",
    "    return pd.read_csv(filename, parse_dates = ['Created Date', 'Closed Date']) # You can specify optional parameters here\n",
    "\n",
    "\n",
    "def read_parallel(directory): # Directory in which split files are located\n",
    "    \n",
    "    # set up your pool\n",
    "    pool = Pool(processes=24) # Num processors found in script\n",
    "\n",
    "    # get a list of split file names\n",
    "    files = os.listdir(directory)\n",
    "    file_list = [(directory+filename) for filename in files if filename.split('.')[1]=='csv']\n",
    "    file_list.sort()\n",
    "\n",
    "    # have your pool map the file names to dataframes\n",
    "    df_list = pool.map(read_csv, file_list)\n",
    "\n",
    "    # reduce the list of dataframes to a single dataframe\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    combined_df = combined_df.drop(0).reset_index(drop=True)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 54.1 s\n"
     ]
    }
   ],
   "source": [
    "data_orig = read_parallel('./311_split/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10min 36s\n"
     ]
    }
   ],
   "source": [
    "data_serial_run = pd.read_csv('./311_Service_Requests_from_2018_to_Present.csv', parse_dates = ['Created Date', 'Closed Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the serial run takes more than 10 times as long to perform the same task!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
